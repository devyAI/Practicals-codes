{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d9575a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install beautifulsoup4 requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f520eea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npagerank_web.py\\nImplements the PageRank algorithm using BeautifulSoup and Python.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "pagerank_web.py\n",
    "Implements the PageRank algorithm using BeautifulSoup and Python.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b608443f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a255888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- CONFIGURATION ----------\n",
    "# You can replace these with any small set of real URLs\n",
    "URLS = [\n",
    "    \"https://en.wikipedia.org/wiki/Web_crawler\",\n",
    "    \"https://en.wikipedia.org/wiki/PageRank\",\n",
    "    \"https://en.wikipedia.org/wiki/Search_engine_optimization\",\n",
    "]\n",
    "DAMPING_FACTOR = 0.85\n",
    "MAX_ITER = 100\n",
    "TOL = 1.0e-6\n",
    "# -----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3635f540",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links(url, all_urls):\n",
    "    \"\"\"\n",
    "    Extracts outgoing links from a webpage that also belong to our URL set.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        res = requests.get(url, timeout=10)\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "        links = set()\n",
    "        for tag in soup.find_all(\"a\", href=True):\n",
    "            href = tag[\"href\"]\n",
    "            # Keep only internal Wikipedia links from our URL set\n",
    "            if href.startswith(\"/wiki/\") and not any(prefix in href for prefix in [\":\", \"#\"]):\n",
    "                full_url = \"https://en.wikipedia.org\" + href\n",
    "                if full_url in all_urls:\n",
    "                    links.add(full_url)\n",
    "        return links\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Failed to fetch {url}: {e}\")\n",
    "        return set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0dbd2ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(urls):\n",
    "    \"\"\"\n",
    "    Builds an adjacency dictionary {url: [linked_urls]}.\n",
    "    \"\"\"\n",
    "    graph = {}\n",
    "    for url in urls:\n",
    "        print(f\"Fetching links from: {url}\")\n",
    "        graph[url] = extract_links(url, urls)\n",
    "    return graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "351f07d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_rank(graph, damping=0.85, max_iter=100, tol=1e-6):\n",
    "    \"\"\"\n",
    "    Computes PageRank using the iterative algorithm:\n",
    "    PR(i) = (1 - d)/N + d * sum(PR(j)/outdeg(j)) for all j linking to i\n",
    "    \"\"\"\n",
    "    nodes = list(graph.keys())\n",
    "    N = len(nodes)\n",
    "    ranks = np.ones(N) / N\n",
    "    index = {nodes[i]: i for i in range(N)}\n",
    "\n",
    "    # Build adjacency matrix\n",
    "    adj = np.zeros((N, N))\n",
    "    for src, out_links in graph.items():\n",
    "        if len(out_links) == 0:\n",
    "            adj[:, index[src]] = 1.0 / N  # Dangling node\n",
    "        else:\n",
    "            for dst in out_links:\n",
    "                if dst in index:\n",
    "                    adj[index[dst], index[src]] = 1.0 / len(out_links)\n",
    "\n",
    "    # Iterate until convergence\n",
    "    for it in range(max_iter):\n",
    "        new_ranks = (1 - damping) / N + damping * adj @ ranks\n",
    "        if np.linalg.norm(new_ranks - ranks, 1) < tol:\n",
    "            print(f\"âœ… Converged after {it+1} iterations\")\n",
    "            break\n",
    "        ranks = new_ranks\n",
    "\n",
    "    return {nodes[i]: ranks[i] for i in range(N)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5c7c91e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Building link graph...\n",
      "Fetching links from: https://en.wikipedia.org/wiki/Web_crawler\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SHREYAS\\AppData\\Local\\Temp\\ipykernel_20204\\172323575.py:7: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(res.text, \"html.parser\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching links from: https://en.wikipedia.org/wiki/PageRank\n",
      "Fetching links from: https://en.wikipedia.org/wiki/Search_engine_optimization\n",
      "\n",
      "ðŸ“Š Graph Structure:\n",
      "https://en.wikipedia.org/wiki/Web_crawler â†’ []\n",
      "https://en.wikipedia.org/wiki/PageRank â†’ []\n",
      "https://en.wikipedia.org/wiki/Search_engine_optimization â†’ []\n",
      "\n",
      "âš™ï¸ Running PageRank...\n",
      "âœ… Converged after 1 iterations\n",
      "\n",
      "ðŸ† === PageRank Scores ===\n",
      "https://en.wikipedia.org/wiki/Web_crawler                                        : 0.3333\n",
      "https://en.wikipedia.org/wiki/PageRank                                           : 0.3333\n",
      "https://en.wikipedia.org/wiki/Search_engine_optimization                         : 0.3333\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ” Building link graph...\")\n",
    "graph = build_graph(URLS)\n",
    "\n",
    "print(\"\\nðŸ“Š Graph Structure:\")\n",
    "for k, v in graph.items():\n",
    "    print(f\"{k} â†’ {list(v)}\")\n",
    "\n",
    "print(\"\\nâš™ï¸ Running PageRank...\")\n",
    "ranks = page_rank(graph, DAMPING_FACTOR, MAX_ITER, TOL)\n",
    "\n",
    "print(\"\\nðŸ† === PageRank Scores ===\")\n",
    "for url, score in sorted(ranks.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{url:80s} : {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6cb4bdce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Converged after 28 iterations\n",
      "\n",
      "Toy Graph PageRank:\n",
      "C : 0.3941\n",
      "A : 0.3725\n",
      "B : 0.1958\n",
      "D : 0.0375\n"
     ]
    }
   ],
   "source": [
    "# If you're offline or Wikipedia access fails, use this sample graph\n",
    "toy_graph = {\n",
    "    \"A\": {\"B\", \"C\"},\n",
    "    \"B\": {\"C\"},\n",
    "    \"C\": {\"A\"},\n",
    "    \"D\": {\"C\"},\n",
    "}\n",
    "\n",
    "toy_ranks = page_rank(toy_graph)\n",
    "print(\"\\nToy Graph PageRank:\")\n",
    "for k, v in sorted(toy_ranks.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{k} : {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f3613b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f0222e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
