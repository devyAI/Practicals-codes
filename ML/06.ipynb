{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy\n",
    "import numpy as np \n",
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the tic tac toe board\n",
    "def display_board(board):\n",
    "    print(\"\\n\".join([\" |\".join(board[i*3:(i+1)*3]) for i in range(3)]))\n",
    "    print(\"-\" * 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there's a winner\n",
    "def check_winner(board, player):\n",
    "    wins = [(0,1,2), (3,4,5), (6,7,8), (0,3,6), (1,4,7), (2,5,8), (0,4,8), (2,4,6)]\n",
    "    return any(all(board[i] == player for i in win) for win in wins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Learning agent functions\n",
    "q_table = {} # Stores Q-values for state-action pairs\n",
    "def get_state(board):\n",
    "    return \"\".join(board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(board, epsilon=0.1):\n",
    "    state = get_state(board)\n",
    "    if random.random() < epsilon or state not in q_table:\n",
    "        return random. choice([i for i, x in enumerate(board) if x == ' '])\n",
    "    return max((i for i, x in enumerate(board) if x == ' '), key=lambda x: q_table[state].get(x, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q(state, action, reward, next_state, alpha=0.5, gamma=0.9):\n",
    "    if state not in q_table:\n",
    "        q_table[state] = {}\n",
    "    old_q = q_table[state].get(action, 0)\n",
    "    next_max = max(q_table.get(next_state, {}).values(), default=0)\n",
    "    q_table[state] [action] = old_q + alpha * (reward + gamma * next_max - old_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play one episode of Tic Tac Toe with Q-learning\n",
    "# board = [' '] * 9\n",
    "# while True:\n",
    "# \tstate = get_state(board)\n",
    "# \taction = choose_action(board)\n",
    "# \tboard[action] = 'X'\n",
    "# \tif check_winner(board, 'X'):\n",
    "# \t\tupdate_q(state, action, 1, get_state(board))\n",
    "# \t\tbreak\n",
    "# \tif ' ' not in board:\n",
    "# \t\tupdate_q(state, action, 0.5, get_state(board))\n",
    "# \t\tbreak\n",
    "# \topp_action = random.choice([i for i, x in enumerate(board) if x == ' '])\n",
    "# \tboard[opp_action] = '0'\n",
    "# \tif check_winner(board, '0'):\n",
    "# \t\tupdate_q(state, action, -1, get_state(board))\n",
    "# \t\tbreak\n",
    "# \tupdate_q(state, action, 0, get_state(board))\n",
    "\n",
    "# Training the AI through reinforcement Learning\n",
    "def train_ai(episodes=5000):\n",
    "\tfor _ in range(episodes):\n",
    "\t\tboard = [' '] * 9\n",
    "\t\twhile True:\n",
    "\t\t\tstate = get_state(board)\n",
    "\t\t\taction = choose_action(board)\n",
    "\t\t\tboard[action] = 'X'\n",
    "\t\t\tif check_winner(board, 'X'):\n",
    "\t\t\t\tupdate_q(state, action, 1, get_state(board))\n",
    "\t\t\t\tbreak\n",
    "\t\t\tif ' ' not in board:\n",
    "\t\t\t\tupdate_q(state, action, 0.5, get_state(board))\n",
    "\t\t\t\tbreak\n",
    "\t\t\topp_action = random.choice([i for i, x in enumerate(board) if x == ' '])\n",
    "\t\t\tboard[opp_action] = '0'\n",
    "\t\t\tif check_winner(board, '0'):\n",
    "\t\t\t\tupdate_q(state, action, -1, get_state(board))\n",
    "\t\t\t\tbreak\n",
    "\t\t\tupdate_q(state, action, 0, get_state(board)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play a game against the trained AI\n",
    "def play_game():\n",
    "\tboard = [' '] * 9\n",
    "\twhile True:\n",
    "\t\tdisplay_board(board)\n",
    "\t\tai_action = choose_action(board, epsilon=0)  # No exploration in test\n",
    "\t\tboard[ai_action] = 'X'\n",
    "\t\tprint(\"\\nAI moved:\")\n",
    "\t\tdisplay_board(board)\n",
    "\t\t\n",
    "\t\tif check_winner(board, 'X'):\n",
    "\t\t\tprint(\"AI wins!\")\n",
    "\t\t\tbreak\n",
    "\t\tif ' ' not in board:\n",
    "\t\t\tprint(\"It's a draw!\")\n",
    "\t\t\tbreak\n",
    "\t\t\n",
    "\t\twhile True:\n",
    "\t\t\ttry:\n",
    "\t\t\t\tplayer_action = int(input(\"\\nEnter your move (0-8): \"))\n",
    "\t\t\t\tif board[player_action] == ' ':\n",
    "\t\t\t\t\tboard[player_action] = '0'\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\t\tprint(\"Invalid move, try again.\")\n",
    "\t\t\texcept ValueError:\n",
    "\t\t\t\tprint(\"Invalid input, try again.\")\n",
    "\t\t\n",
    "\t\tif check_winner(board, '0'):\n",
    "\t\t\tdisplay_board(board)\n",
    "\t\t\tprint(\"You win!\")\n",
    "\t\t\tbreak\n",
    "\t\tif ' ' not in board:\n",
    "\t\t\tdisplay_board(board)\n",
    "\t\t\tprint(\"It's a draw!\")\n",
    "\t\t\tbreak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AI...\n",
      "Training completed. Let's play!\n",
      "game starts! you are 0 and ai is X\n",
      "positions ((0-8))\n",
      "[[0 1 2]\n",
      " [3 4 5]\n",
      " [6 7 8]]\n"
     ]
    }
   ],
   "source": [
    "#main\n",
    "print(\"Training AI...\")\n",
    "train_ai()\n",
    "print(\"Training completed. Let's play!\")\n",
    "print(\"game starts! you are 0 and ai is X\")\n",
    "print(\"positions ((0-8))\")\n",
    "print(np.arange(9).reshape(3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  |  | \n",
      "  |  | \n",
      "  |  | \n",
      "---------\n",
      "\n",
      "AI moved:\n",
      "X |  | \n",
      "  |  | \n",
      "  |  | \n",
      "---------\n",
      "X |0 | \n",
      "  |  | \n",
      "  |  | \n",
      "---------\n",
      "\n",
      "AI moved:\n",
      "X |0 | \n",
      "X |  | \n",
      "  |  | \n",
      "---------\n",
      "X |0 | \n",
      "X |  | \n",
      "0 |  | \n",
      "---------\n",
      "\n",
      "AI moved:\n",
      "X |0 | \n",
      "X |X | \n",
      "0 |  | \n",
      "---------\n",
      "X |0 | \n",
      "X |X | \n",
      "0 |0 | \n",
      "---------\n",
      "\n",
      "AI moved:\n",
      "X |0 | \n",
      "X |X |X\n",
      "0 |0 | \n",
      "---------\n",
      "AI wins!\n"
     ]
    }
   ],
   "source": [
    "play_game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
